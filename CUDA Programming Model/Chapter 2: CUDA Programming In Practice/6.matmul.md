## Matrix Multiplication Example
[Back to Table of Content](../../Readme.md) | [Previous: CUDA Syntax](5.cuda_syntax.md) | **[Next:]()**

![Matmul-img](./imgs/serial-matmul.png)
We have discussed the GPU increment example extenstively before. To reinforce our knowledge, let's work another famous example which is dense matrix-matrix multiplication. In the classic matrix-matrix multiply problem given Matrices A of dimensionality mxk and and B with dimensionality kxn we want to compute the the matrix C which is mxn. Above you can see a serial matmul code (there A is HA x WA, B is HB x WB and C is HC x WC, note that HC = HA, WB = WC, WA = HB ).

### Host code
The code below shows the host code for a our first version of matmul over this course. The steps are similar to the general host template we discussed before ([Necessary Steps for Host-Device Heterogeneous Code](4.steps_Host_Device_Code.md)). First we allocate the matrices in on CPU and randomly initialize A and B in steps 1,2. Then in step 3, we allocate memory on the device for the matrices. The naming convention is that host arrays' pointers start with `h_` and device arrays' pointers start with `d_`. Another general point worth to mention is that device pointers of the arrays are all residing in the CPU as GPU does not have an operating system, CPU takes care of its memory address. In step 4 we copy the matrices A and B from host to device via the `cudamemcpy` API. Step 5 is the most essential step as we are indicating our work strategy via defining the thread hierarchy parameters. We will go through this step and why we chose this parameters next. Step 6 is just calling our matmul kernel which compute the results in the Device; Note that kernel calls are asyncronous in the host meaning that the CPU won't wait on that to complete, so normaly we add `cudaDeviceSynchronize();` right after it. However, there is an implicit call to the `cudaDeviceSynchronize();` in the cudaMemcpy() under step 7 so we don't need to explicitly mention it. Step 7 is bringing the results from the device to the host. Finally, step 8 is the memory management, we need to clean both Host and Device memory.  

```c
// 1. allocate host memory
int size_A = WA * HA; 
int mem_size_A = sizeof(float) * size_A;
float* h_A = (float*) malloc(mem_size_A);

int size_B = WB * HB; 
int mem_size_B = sizeof(float) * size_B;
float* h_B = (float*) malloc(mem_size_B);

int size_C = WC * HC; 
int mem_size_C = sizeof(float) * size_C;
float* h_C = (float*) malloc(mem_size_C);

// 2. initialize host memory
randomInit(h_A, size_A); 
randomInit(h_B, size_B);

// 3. allocate device memory
float* d_A; 
cudaMalloc((void**) &d_A, mem_size_A);

float* d_B; 
cudaMalloc((void**) &d_B, mem_size_B);

float* d_C; 
cudaMalloc((void**) &d_C, mem_size_C);

// 4. copy host memory to device
cudaMemcpy(d_A, h_A, mem_size_A, cudaMemcpyHostToDevice);
cudaMemcpy(d_B, h_B, mem_size_B, cudaMemcpyHostToDevice);

// 5. setup execution parameters
dim3 threads(BLOCK_SIZE, BLOCK_SIZE);
dim3 grid(ceil(WC / (float)threads.x), ceil(HC /(float) threads.y));

// 6. execute the kernel
matrixMul<<< grid, threads >>>(d_C, d_A, d_B, HA, WA, WB);

// 7. copy result from device to host
cudaMemcpy(h_C, d_C, mem_size_C, cudaMemcpyDeviceToHost);

// 8. clean up memory
free(h_A); 
free(h_B); 
free(h_C);
cudaFree(d_A); 
cudaFree(d_B); 
cudaFree(d_C);
```

### Thread hierarchy
- The main idea for he work sharing between threads for this matmul example is: **"each thread computes a single element of the output matrix."**
- In the figure below we see matrices A, B, and C in green. 
- Blue blocks are indicating the thread blocks (some boundary thread blocks are idle as there is no element for them to process).
- We define BLOCK_SIZE x BLOCK_SIZE 2D thread blocks, each responsible to compute a BLOCK_SIZE x BLOCK_SIZE block of the C marix. That is why in the host code we have `dim3 threads(BLOCK_SIZE, BLOCK_SIZE);`
- we setup a 2D grid of thread blocks by dividing the dimensions of C matrix to BLOCK_SIZE like `dim3 grid(ceil(WC / (float)threads.x), ceil(HC /(float) threads.y));`. Note that we ceil function to make sure that all the elements in the result matrix are covered, this may leave some of the threads in the boundary threadblocks idle if the dimension of the C matrix are not perfect multiple of BLOCK_SIZE like the figure below.

![Matmul-img](./imgs/matmul-img.png)

### Kernel code (Device)
- Now that we designed the threads work strategy and thread hierarchy, we can write the kernel code (code section below). 
- Recall that he kernel code is executed by all the threads. So each threads first to figure out what is the work that it is responsible of. In this example as we discussed, each threads has to compute a single element in the result matrix, hence it should compute the indeces of the element. This is done in step 1,2 in the kernel code where the thread computes the row and col of the element.
- Step 3 starts with boundary check to make sure that the index computed by the thread actually exists and is not out of boundary. Finally the thread computes the result element by doing a dot product of the corresponding row of A and column of B.


```c
__global__ void mmkernel(float* A, float* B, float* C, int hA, int wA, int wB)
{
    // 1. Calculate row index of the elt. in A and C
    int row= blockIdx.y*blockDim.y+threadIdx.y;

    // 2. Calculate the column index for B and C
    int col = blockIdx.x*blockDim.x+threadIdx.x;

    // 3. boundary chech
    if ((row < hA) && (col < wB))
    { 
        float Pvalue = 0;
        // thread computes 1 elt. of block sub-matrix
        for (int k = 0; k < wA; ++k)
        { 
            Pvalue += A[row*wA+k]*B[k*wB+col]; 
        }
        C[row*wB+col] = Pvalue;
    }
}
```
