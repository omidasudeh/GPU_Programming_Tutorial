## Work Division: CPUs vs GPUs
[Back to Table of Content](../../Readme.md) | [Previous: Synchronization in GPU Threads](7.synchronization.md) | **[Next: SIMT vs SIMD: A Key Difference](9.simt_vs_simd.md)**

CPUs typically divide work into a few large chunks, with the number of threads being in the same order as the number of cores. 

GPUs, however, employ **data parallel programming**, meaning there is a 1-to-1 relation between threads and data elements. GPUs achieve their high parallelism by having incredibly fast context switches between warps (each warp has dedicated registers, so there is no need to save and load contexts).

GPU programming models are similar to OpenMP's **"thread-ID partitioning"** where each thread knows its responsibilities based on its ID. However, OpenMP uses a *work-sharing model*, automatically dividing loops among threads, which is different from GPU's explicit thread management.

![OMP-thread-ID partitioning](./imgs/OMP-thread-ID%20partitioning.png)

[Back to Table of Content](../../Readme.md) | [Previous: Synchronization in GPU Threads](7.synchronization.md) | **[Next: SIMT vs SIMD: A Key Difference](9.simt_vs_simd.md)**